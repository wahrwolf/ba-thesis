# What I did
I trained 4 neural networks with 2 variations of the same source text and compared the BLEU score.
All models were trained on an merged corpus of  parts of the EMA, Europarl and ECB corpora from the opencorpus project.

The first two models were trained on the language pair DE-EN and the second set on CS-EN.
In both sets was one model trained on the plain text corpus and the second one on a modified corpus, which contained a Source annotation.

Those annotations where unique to the corpus.

We used approx 35000 examples per model and archived a validation accurcaczy around 35%.

# Why I did it
With the 

# What variation I did


